import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib import cmimport seaborn as snsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler, LabelEncoderfrom sklearn.neural_network import MLPClassifierfrom imblearn.over_sampling import SMOTEfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score,confusion_matrix, recall_score, precision_score, classification_report, average_precision_score, roc_curve, roc_auc_score, mean_squared_error, mean_absolute_error, multilabel_confusion_matrix, log_lossdef outlier(df, col):    Q1, Q3 = np.quantile(df[col], [0.0, 0.80])    IQR = Q3 - Q1    min = Q1 - IQR*1.5    max = Q3 + IQR*1.5    return df[(df[col] >= min) & (df[col] <= max)]def ApplyEncoder(OriginalColumn):     global df    Encoder = LabelEncoder()    Encoder.fit(df[OriginalColumn])    return Encoder.transform(df[OriginalColumn])# Data Acquisitiondf = pd.read_csv('creditcard.csv')# Data Understandingprint("Size of the UCI Credit Card Dataset: " + str(df.shape))print(df.dtypes) # to identify the datatypes, identify the categorical datatype values to convert to dummy variablesprint("Data Description")nullAttributes = df.isnull().sum()print(df.describe().transpose())print("Dataset Info")print(df.info())missingVals = df.isna().sum()print("Missing Values")print(missingVals) # observe missing values in the datasetprint("Distribution of Target")print(df["Approved"].value_counts())  #observe the distribution of approval# removing outliers in the datasetnum_cols = df.columns[df.dtypes != 'object']for i in num_cols:    df = outlier(df, i)    df[df.duplicated()] #determine whether contains duplicatesobj_cols = df.columns[df.dtypes == "object"]list(obj_cols)# Label Encodingencoder = LabelEncoder()for col in obj_cols:    df[col] = ApplyEncoder(col)# separating target attribute (y) from rest of the factors (x)x = df.iloc[:,0:15]y = df.iloc[:, 15]x=pd.get_dummies(x)# SMOTE - to test whether this improves the metrics - if doesn't can removesmt = SMOTE(sampling_strategy='not majority')x, y = smt.fit_resample(x, y)# Creating Dummy Attributes/one hot encoding - not sure whether necessary, test to see impactx=pd.get_dummies(x)# Data Scalingsc = StandardScaler()x = sc.fit_transform(x)# to verify effects of preprocessing and scalingprint(df.describe().transpose())x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=30,shuffle=True)